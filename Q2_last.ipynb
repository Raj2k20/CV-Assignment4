{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z5aTJze4OnA",
        "outputId": "228f7af0-9859-4c83-d772-e22a88ae4c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions for /content/drive/MyDrive/Images/banana1.jpg:\n",
            "banana: 0.77447\n",
            "lemon: 0.03464\n",
            "slug: 0.03345\n",
            "butternut squash: 0.01043\n",
            "tusker: 0.00899\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/banana2.jpg:\n",
            "banana: 0.96961\n",
            "lemon: 0.00512\n",
            "acorn squash: 0.00333\n",
            "slug: 0.00144\n",
            "bib: 0.00140\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bicycle1.jpg:\n",
            "tandem bicycle: 0.57439\n",
            "mountain bike: 0.15140\n",
            "disc brake: 0.05778\n",
            "unicycle: 0.04666\n",
            "tricycle: 0.03666\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bicycle2.jpeg:\n",
            "mountain bike: 0.75485\n",
            "tandem bicycle: 0.12317\n",
            "unicycle: 0.06895\n",
            "tricycle: 0.01487\n",
            "mongoose: 0.00670\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bird1.jpg:\n",
            "macaw: 0.16383\n",
            "hen: 0.11998\n",
            "jacamar: 0.06347\n",
            "duck: 0.05227\n",
            "bulbul: 0.04126\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bird2.jpg:\n",
            "hen: 0.13121\n",
            "duck: 0.07141\n",
            "common gallinule: 0.04753\n",
            "bulbul: 0.04612\n",
            "drumstick: 0.04366\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/car1.jpg:\n",
            "minibus: 0.28072\n",
            "tow truck: 0.06397\n",
            "taxicab: 0.06097\n",
            "car wheel: 0.05897\n",
            "minivan: 0.05241\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/car2.jpg:\n",
            "taxicab: 0.15552\n",
            "minibus: 0.08431\n",
            "minivan: 0.08384\n",
            "race car: 0.04765\n",
            "station wagon: 0.04646\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/coffeemug1.jpg:\n",
            "espresso: 0.28693\n",
            "coffee mug: 0.28597\n",
            "coffeemaker: 0.21093\n",
            "cup: 0.05311\n",
            "pitcher: 0.04541\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/coffeemug2.jpg:\n",
            "coffee mug: 0.36748\n",
            "espresso: 0.10185\n",
            "cup: 0.06337\n",
            "coffeemaker: 0.05699\n",
            "teapot: 0.04032\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/dog1.jpg:\n",
            "Beagle: 0.16741\n",
            "Labrador Retriever: 0.12492\n",
            "Toy Poodle: 0.05779\n",
            "Norfolk Terrier: 0.04738\n",
            "Lakeland Terrier: 0.03023\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/dog2.jpg:\n",
            "Chihuahua: 0.24932\n",
            "Labrador Retriever: 0.15105\n",
            "toy terrier: 0.06641\n",
            "Saluki: 0.06565\n",
            "Weimaraner: 0.05540\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/fish1.png:\n",
            "toucan: 0.11274\n",
            "jacamar: 0.09488\n",
            "lorikeet: 0.09027\n",
            "macaw: 0.03031\n",
            "tench: 0.03026\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/fish2.jpg:\n",
            "goldfish: 0.55970\n",
            "tench: 0.26453\n",
            "clownfish: 0.08325\n",
            "pufferfish: 0.01538\n",
            "orange: 0.00719\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/flower1.jpg:\n",
            "rose hip: 0.08945\n",
            "strawberry: 0.05081\n",
            "bell pepper: 0.02948\n",
            "barrette: 0.02841\n",
            "spindle: 0.02714\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/flower2.jpg:\n",
            "daisy: 0.15994\n",
            "pinwheel: 0.10963\n",
            "paddle wheel: 0.07689\n",
            "barrette: 0.05443\n",
            "sea anemone: 0.02873\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/guitar1.png:\n",
            "acoustic guitar: 0.91748\n",
            "plectrum: 0.01718\n",
            "electric guitar: 0.01644\n",
            "banjo: 0.00895\n",
            "maraca: 0.00410\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/guitar2.png:\n",
            "acoustic guitar: 0.76977\n",
            "electric guitar: 0.08041\n",
            "banjo: 0.06216\n",
            "maraca: 0.01576\n",
            "plectrum: 0.00646\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/laptop1.jpg:\n",
            "laptop computer: 0.51025\n",
            "hand-held computer: 0.14520\n",
            "notebook computer: 0.09295\n",
            "computer keyboard: 0.08295\n",
            "desktop computer: 0.06202\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/laptop2.jpg:\n",
            "laptop computer: 0.50645\n",
            "notebook computer: 0.20729\n",
            "desktop computer: 0.10585\n",
            "monitor: 0.03535\n",
            "hand-held computer: 0.02899\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"RN50\", device=device)\n",
        "\n",
        "# Load ImageNet labels from JSON\n",
        "with open(\"/content/drive/MyDrive/imagenet_labels.json\", \"r\") as f:\n",
        "    classes = json.load(f)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image_input = transform(image).unsqueeze(0).to(device)\n",
        "    return image_input\n",
        "\n",
        "def predict_image(image_path, classes, top_k=5):\n",
        "    image_input = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip.tokenize(classes).to(device)\n",
        "        logits_per_image, logits_per_text = model(image_input, text_features)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        # Display top categories\n",
        "        top_k_idx = np.argsort(probs.squeeze())[::-1][:top_k]\n",
        "        for idx in top_k_idx:\n",
        "            print(f\"{classes[idx]}: {probs.squeeze()[idx]:.5f}\")\n",
        "\n",
        "# Example images\n",
        "example_images = [\"/content/drive/MyDrive/Images/banana1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/banana2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/bicycle1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/bicycle2.jpeg\",\n",
        "    \"/content/drive/MyDrive/Images/bird1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/bird2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/car1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/car2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/coffeemug1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/coffeemug2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/dog1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/dog2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/fish1.png\",\n",
        "    \"/content/drive/MyDrive/Images/fish2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/flower1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/flower2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/guitar1.png\",\n",
        "    \"/content/drive/MyDrive/Images/guitar2.png\",\n",
        "    \"/content/drive/MyDrive/Images/laptop1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/laptop2.jpg\",]\n",
        "\n",
        "# Test with example images\n",
        "for image_path in example_images:\n",
        "    print(f\"Predictions for {image_path}:\")\n",
        "    predict_image(image_path, classes)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYPJljxF8S8b",
        "outputId": "df0f2b41-df28-48bf-dc80-ae4675dfe07a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions for /content/drive/MyDrive/Images/banana1.jpg:\n",
            "nipple: 0.32817\n",
            "banana: 0.07244\n",
            "hook: 0.07063\n",
            "toilet seat: 0.04974\n",
            "rocking chair: 0.04221\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/banana2.jpg:\n",
            "flatworm: 0.49722\n",
            "banana: 0.31156\n",
            "sea slug: 0.01801\n",
            "pitcher: 0.01108\n",
            "pan flute: 0.00937\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bicycle1.jpg:\n",
            "mountain bike: 0.66677\n",
            "tandem bicycle: 0.13627\n",
            "tricycle: 0.05243\n",
            "disc brake: 0.03757\n",
            "turnstile: 0.01667\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bicycle2.jpeg:\n",
            "tricycle: 0.87903\n",
            "mountain bike: 0.04611\n",
            "tandem bicycle: 0.02420\n",
            "moped: 0.02302\n",
            "unicycle: 0.00658\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bird1.jpg:\n",
            "crash helmet: 0.06224\n",
            "pinwheel: 0.05892\n",
            "pencil sharpener: 0.04948\n",
            "barrette: 0.04848\n",
            "nipple: 0.02645\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/bird2.jpg:\n",
            "laptop computer: 0.22874\n",
            "envelope: 0.15189\n",
            "website: 0.12267\n",
            "comic book: 0.11740\n",
            "jigsaw puzzle: 0.03795\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/car1.jpg:\n",
            "go-kart: 0.28669\n",
            "forklift: 0.14004\n",
            "lawn mower: 0.12846\n",
            "pencil sharpener: 0.08113\n",
            "vacuum cleaner: 0.05068\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/car2.jpg:\n",
            "comic book: 0.06635\n",
            "ocarina: 0.06322\n",
            "hockey puck: 0.03564\n",
            "analog clock: 0.03321\n",
            "wall clock: 0.03094\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/coffeemug1.jpg:\n",
            "computer mouse: 0.25053\n",
            "envelope: 0.16458\n",
            "bib: 0.07406\n",
            "remote control: 0.06407\n",
            "soap dispenser: 0.05328\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/coffeemug2.jpg:\n",
            "cup: 0.59378\n",
            "coffee mug: 0.34415\n",
            "water jug: 0.00578\n",
            "pitcher: 0.00458\n",
            "pencil sharpener: 0.00388\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/dog1.jpg:\n",
            "envelope: 0.77786\n",
            "T-shirt: 0.03932\n",
            "dust jacket: 0.01558\n",
            "bib: 0.01092\n",
            "carton: 0.00819\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/dog2.jpg:\n",
            "Band-Aid: 0.13403\n",
            "pencil case: 0.12025\n",
            "pencil sharpener: 0.06876\n",
            "jigsaw puzzle: 0.05908\n",
            "nipple: 0.03163\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/fish1.png:\n",
            "parachute: 0.09014\n",
            "swimming cap: 0.07402\n",
            "umbrella: 0.06376\n",
            "sarong: 0.05179\n",
            "piggy bank: 0.03120\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/fish2.jpg:\n",
            "whistle: 0.34935\n",
            "pinwheel: 0.06262\n",
            "goldfish: 0.05553\n",
            "pencil sharpener: 0.05224\n",
            "nipple: 0.05168\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/flower1.jpg:\n",
            "barrette: 0.91383\n",
            "velvet: 0.01068\n",
            "handkerchief: 0.00883\n",
            "eraser: 0.00616\n",
            "wool: 0.00501\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/flower2.jpg:\n",
            "barrette: 0.24796\n",
            "envelope: 0.16174\n",
            "electric fan: 0.06877\n",
            "doormat: 0.06718\n",
            "Band-Aid: 0.04742\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/guitar1.png:\n",
            "acoustic guitar: 0.99653\n",
            "plectrum: 0.00123\n",
            "electric guitar: 0.00108\n",
            "microphone: 0.00055\n",
            "violin: 0.00010\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/guitar2.png:\n",
            "acoustic guitar: 0.94424\n",
            "plectrum: 0.05260\n",
            "electric guitar: 0.00156\n",
            "violin: 0.00025\n",
            "pan flute: 0.00017\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/laptop1.jpg:\n",
            "analog clock: 0.29219\n",
            "notebook computer: 0.22355\n",
            "hand-held computer: 0.14551\n",
            "space bar: 0.08545\n",
            "wall clock: 0.05117\n",
            "\n",
            "Predictions for /content/drive/MyDrive/Images/laptop2.jpg:\n",
            "notebook computer: 0.74181\n",
            "laptop computer: 0.13936\n",
            "desktop computer: 0.05054\n",
            "CRT screen: 0.01600\n",
            "hand-held computer: 0.01497\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load ImageNet pretrained ResNet-50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load ImageNet labels from JSON\n",
        "with open(\"/content/drive/MyDrive/archive/imagenet_labels.json\", \"r\") as f:\n",
        "    classes = json.load(f)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image_input = transform(image).unsqueeze(0).to(device)\n",
        "    return image_input\n",
        "\n",
        "def predict_image(image_path, classes, top_k=5):\n",
        "    image_input = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_input)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=-1)[0]\n",
        "\n",
        "        # Display top categories\n",
        "        top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
        "        for prob, idx in zip(top_k_probs, top_k_indices):\n",
        "            print(f\"{classes[idx]}: {prob.item():.5f}\")\n",
        "\n",
        "# Example images\n",
        "example_images = [\"/content/drive/MyDrive/Images/banana1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/banana2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/bicycle1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/bicycle2.jpeg\",\n",
        "    \"/content/drive/MyDrive/Images/bird1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/bird2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/car1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/car2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/coffeemug1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/coffeemug2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/dog1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/dog2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/fish1.png\",\n",
        "    \"/content/drive/MyDrive/Images/fish2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/flower1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/flower2.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/guitar1.png\",\n",
        "    \"/content/drive/MyDrive/Images/guitar2.png\",\n",
        "    \"/content/drive/MyDrive/Images/laptop1.jpg\",\n",
        "    \"/content/drive/MyDrive/Images/laptop2.jpg\",]\n",
        "\n",
        "# Test with example images\n",
        "for image_path in example_images:\n",
        "    print(f\"Predictions for {image_path}:\")\n",
        "    predict_image(image_path, classes)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxwi1x7P8pX3"
      },
      "source": [
        "| Image | CLIP | Imagenet Pretained RN50 |\n",
        "|-----------------|-----------------|-----------------|\n",
        "| Banana1    | banana: 0.77447    | banana: 0.07244   |\n",
        "| Banana2    | banana: 0.96961   | banana: 0.31156    |\n",
        "| Bicycle1    | tandem bicycle: 0.57439    | **Not Detected**    |\n",
        "| Bicycle2    | mountain bike: 0.75485   | tandem bicycle: 0.02420    |\n",
        "| Bird1    | hen: 0.13121   | **Not Detected**   |\n",
        "| Bird2    |common gallinule: 0.04753   | **Not Detected**    |\n",
        "| Car1    | minibus: 0.28072    | **Not Detected**    |\n",
        "| Car2    | taxicab: 0.15552   | **Not Detected**    |\n",
        "| Coffeemug1    | coffee mug: 0.28597    | **Not Detected**    |\n",
        "| Coffeemug2    | coffee mug: 0.36748  | coffee mug: 0.34415    |\n",
        "| Dog1    | Beagle: 0.16741    | **Not Detected**    |\n",
        "| Dog2    | Chihuahua: 0.24932    | **Not Detected**    |\n",
        "| Fish1    | tench: 0.03026    | **Not Detected**   |\n",
        "| Fish2    | goldfish: 0.55970   | **Not Detected**    |\n",
        "| Flower1    | **Not Detected**    | **Not Detected**    |\n",
        "| Flower2    | daisy: 0.15994    | **Not Detected**    |\n",
        "| Guitar1    | acoustic guitar: 0.91748  | acoustic guitar: 0.99653    |\n",
        "| Guitar2    | acoustic guitar: 0.76977    | acoustic guitar: 0.94424    |\n",
        "| Laptop1    |laptop computer: 0.51025    | notebook computer: 0.22355    |\n",
        "| Laptop2    |laptop computer: 0.50645    | laptop computer: 0.13936    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEVciSDVDpWH"
      },
      "source": [
        "(i) Banana and Bicycle are detected better using CLIP but not by ImageNet pretrained RN50.\n",
        "\n",
        "Reason:\n",
        "CLIP can understand abstract concepts and relationships between objects depicted in images and text. The cartoon banana might not be recognizable based on visual features alone, but CLIP can leverage textual context to understand its representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI7UEqWqEHEy"
      },
      "source": [
        "(ii) Guitar is well detected using ImageNet pretained RN50 than CLIP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP2YMI9lEYxf"
      },
      "source": [
        "# Part 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRymwJ8eHHzF",
        "outputId": "305d0b90-2b92-4a85-d489-f21b3a0ac051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhU-0oBQk0T"
      },
      "source": [
        "## (i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-o4Wvuh9LEG",
        "outputId": "8f649759-0931-4a77-962e-ec44f85bc4d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/openai_clip_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP16 Mean Time: 0.018815 sec (std: 0.093690 sec)\n",
            "FP32 Mean Time: 0.011019 sec (std: 0.001439 sec)\n",
            "Difference in Mean Time: -0.007797 sec\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Load the original RN50 CLIP image encoder model trained with FP32 precision\n",
        "model_fp32, preprocess = torch.hub.load('openai/clip', 'RN50')\n",
        "model_fp32.eval()\n",
        "\n",
        "# Convert the model's parameters to FP16\n",
        "model_fp16 = model_fp32.half()\n",
        "\n",
        "# Define image preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize image to match model input size\n",
        "    transforms.CenterCrop(224),  # Crop image to center\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "])\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/Images/coffeemug2.jpg'\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Move the input tensor to the same device as the model's parameters\n",
        "device = next(model_fp16.parameters()).device\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "# Measure the time taken to encode an image using the FP16 model\n",
        "runs = 100\n",
        "fp16_times = []\n",
        "for _ in range(runs):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model_fp16.encode_image(image_tensor)\n",
        "    end_time = time.time()\n",
        "    fp16_times.append(end_time - start_time)\n",
        "\n",
        "# Calculate mean and standard deviation for FP16 model\n",
        "fp16_mean_time = torch.tensor(fp16_times).mean().item()\n",
        "fp16_std_time = torch.tensor(fp16_times).std().item()\n",
        "\n",
        "# Move the input tensor to the same device as the model's parameters for FP32 model\n",
        "image_tensor = image_tensor.to(next(model_fp32.parameters()).device)\n",
        "\n",
        "# Measure the time taken to encode an image using the original FP32 model\n",
        "fp32_times = []\n",
        "for _ in range(runs):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model_fp32.encode_image(image_tensor)\n",
        "    end_time = time.time()\n",
        "    fp32_times.append(end_time - start_time)\n",
        "\n",
        "# Calculate mean and standard deviation for FP32 model\n",
        "fp32_mean_time = torch.tensor(fp32_times).mean().item()\n",
        "fp32_std_time = torch.tensor(fp32_times).std().item()\n",
        "\n",
        "# Calculate the difference in mean wall-clock times between the FP32 and FP16 models\n",
        "time_difference = fp32_mean_time - fp16_mean_time\n",
        "\n",
        "# Report results\n",
        "print(\"FP16 Mean Time: {:.6f} sec (std: {:.6f} sec)\".format(fp16_mean_time, fp16_std_time))\n",
        "print(\"FP32 Mean Time: {:.6f} sec (std: {:.6f} sec)\".format(fp32_mean_time, fp32_std_time))\n",
        "print(\"Difference in Mean Time: {:.6f} sec\".format(time_difference))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOvR1MS8QoM0"
      },
      "source": [
        "## (ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w2pHc1oP9zI",
        "outputId": "87aa12ef-f64d-4e03-e895-c525748fdecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/openai_clip_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: dog\n",
            "FP16 Probabilities: [0.000966  0.000996  0.000978  ... 0.0009494 0.000998  0.001164 ]\n",
            "\n",
            "Class: cat\n",
            "FP16 Probabilities: [0.0009513 0.001052  0.000946  ... 0.00099   0.000953  0.001201 ]\n",
            "\n",
            "Class: car\n",
            "FP16 Probabilities: [0.00093   0.000991  0.0009274 ... 0.000992  0.000992  0.001016 ]\n",
            "\n",
            "Class: flower\n",
            "FP16 Probabilities: [0.000952 0.001027 0.000952 ... 0.000962 0.000982 0.001016]\n",
            "\n",
            "Class: bird\n",
            "FP16 Probabilities: [0.000981  0.00096   0.000982  ... 0.0009503 0.001046  0.001113 ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the original RN50 CLIP image encoder model trained with FP32 precision\n",
        "model_fp32, preprocess = torch.hub.load('openai/clip', 'RN50')\n",
        "model_fp32.eval()\n",
        "\n",
        "# Convert the model's parameters to FP16\n",
        "model_fp16 = model_fp32.half()\n",
        "model_fp16.to('cuda')  # Move the model to GPU if available\n",
        "\n",
        "# Define image preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize image to match model input size\n",
        "    transforms.CenterCrop(224),  # Crop image to center\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "])\n",
        "\n",
        "# Define classes for the images\n",
        "classes = ['dog', 'cat', 'car', 'flower', 'bird']\n",
        "\n",
        "# Load and preprocess the images\n",
        "image_paths = ['/content/drive/MyDrive/Images/dog1.jpg', '/content/drive/MyDrive/Images/cat.jpeg', '/content/drive/MyDrive/Images/car1.jpg', '/content/drive/MyDrive/Images/flower1.jpg', '/content/drive/MyDrive/Images/bird1.jpg']\n",
        "images = [Image.open(path) for path in image_paths]\n",
        "image_tensors = torch.stack([transform(image) for image in images]).to('cuda')  # Move the input tensor to GPU if available\n",
        "\n",
        "# Calculate probabilities using FP16 model\n",
        "with torch.no_grad():\n",
        "    logits_fp16 = model_fp16.encode_image(image_tensors.half())\n",
        "\n",
        "probabilities_fp16 = torch.nn.functional.softmax(logits_fp16, dim=-1)\n",
        "\n",
        "# Convert probabilities to numpy arrays for easier comparison\n",
        "probabilities_fp16 = probabilities_fp16.cpu().numpy()\n",
        "\n",
        "# Print class labels and corresponding probabilities for FP16 model\n",
        "for i, class_name in enumerate(classes):\n",
        "    print(f\"Class: {class_name}\")\n",
        "    print(f\"FP16 Probabilities: {probabilities_fp16[i]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SyMumQRQZRh",
        "outputId": "0ee86f1f-6219-4cf7-e846-b6d25a251f72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/openai_clip_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: dog\n",
            "FP32 Probabilities: [0.000966  0.000996  0.000978  ... 0.0009494 0.000998  0.001164 ]\n",
            "\n",
            "Class: cat\n",
            "FP32 Probabilities: [0.0009513 0.001052  0.000946  ... 0.00099   0.000953  0.001201 ]\n",
            "\n",
            "Class: car\n",
            "FP32 Probabilities: [0.00093   0.000991  0.0009274 ... 0.000992  0.000992  0.001016 ]\n",
            "\n",
            "Class: flower\n",
            "FP32 Probabilities: [0.000952 0.001027 0.000952 ... 0.000962 0.000982 0.001016]\n",
            "\n",
            "Class: bird\n",
            "FP32 Probabilities: [0.000981  0.00096   0.000982  ... 0.0009503 0.001046  0.001113 ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the original RN50 CLIP image encoder model trained with FP32 precision\n",
        "model_fp32, preprocess = torch.hub.load('openai/clip', 'RN50')\n",
        "model_fp32.eval()\n",
        "\n",
        "# Convert the model's parameters to FP16\n",
        "model_fp16 = model_fp32.half()\n",
        "model_fp16.to('cuda')  # Move the model to GPU if available\n",
        "\n",
        "# Define image preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize image to match model input size\n",
        "    transforms.CenterCrop(224),  # Crop image to center\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "])\n",
        "\n",
        "# Define classes for the images\n",
        "classes = ['dog', 'cat', 'car', 'flower', 'bird']\n",
        "\n",
        "# Load and preprocess the images\n",
        "image_paths = ['/content/drive/MyDrive/Images/dog1.jpg', '/content/drive/MyDrive/Images/cat.jpeg', '/content/drive/MyDrive/Images/car1.jpg', '/content/drive/MyDrive/Images/flower1.jpg', '/content/drive/MyDrive/Images/bird1.jpg']\n",
        "images = [Image.open(path) for path in image_paths]\n",
        "image_tensors = torch.stack([transform(image) for image in images]).to('cuda')  # Move the input tensor to GPU if available\n",
        "\n",
        "# Calculate probabilities using FP32 model\n",
        "with torch.no_grad():\n",
        "    logits_fp32 = model_fp32.encode_image(image_tensors)\n",
        "\n",
        "probabilities_fp32 = torch.nn.functional.softmax(logits_fp32, dim=-1)\n",
        "\n",
        "# Convert probabilities to numpy arrays for easier comparison\n",
        "probabilities_fp32 = probabilities_fp32.cpu().numpy()\n",
        "\n",
        "# Print class labels and corresponding probabilities for FP32 model\n",
        "for i, class_name in enumerate(classes):\n",
        "    print(f\"Class: {class_name}\")\n",
        "    print(f\"FP32 Probabilities: {probabilities_fp32[i]}\")\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g11MPKiRjMS"
      },
      "source": [
        "- There is no much difference between FP32 and FP16\n",
        "- It seems that there are no significant differences between the probabilities calculated using the FP32 and FP16 models. The maximum absolute difference across all classes is 0.0, indicating that the probabilities are exactly the same for both models.\n",
        "- This result suggests that converting the model's parameters to FP16 did not introduce any noticeable differences in the output probabilities. However, this may not always be the case with other models or datasets. In some cases, using FP16 may lead to slight differences in the output due to the reduced precision, but in this particular scenario, the differences are negligible.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8NdK8zqRfuR"
      },
      "source": [
        "## (iii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIdjm9VoRHX1",
        "outputId": "a5e99025-58af-48f8-fc11-9a9e5ade8cff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/openai_clip_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory usage before FP32 forward pass: 1247 MiB\n",
            "Memory usage after FP32 forward pass: 1247 MiB\n",
            "Memory usage before FP16 forward pass: 1247 MiB\n",
            "Memory usage after FP16 forward pass: 1247 MiB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import subprocess\n",
        "\n",
        "# Function to run nvidia-smi command and parse memory usage\n",
        "def get_gpu_memory_usage():\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], stdout=subprocess.PIPE)\n",
        "    memory_usage = int(result.stdout.decode().strip())\n",
        "    return memory_usage\n",
        "\n",
        "# Load the original RN50 CLIP image encoder model trained with FP32 precision\n",
        "model_fp32, preprocess = torch.hub.load('openai/clip', 'RN50')\n",
        "model_fp32.eval()\n",
        "model_fp32.to('cuda')  # Move the model to GPU if available\n",
        "\n",
        "# Convert the model's parameters to FP16\n",
        "model_fp16 = model_fp32.half()\n",
        "\n",
        "# Define image preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize image to match model input size\n",
        "    transforms.CenterCrop(224),  # Crop image to center\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "])\n",
        "\n",
        "# Define classes for the images\n",
        "classes = ['dog', 'cat', 'car', 'flower', 'bird']\n",
        "\n",
        "# Load and preprocess the images\n",
        "image_paths = ['/content/drive/MyDrive/Images/dog1.jpg', '/content/drive/MyDrive/Images/cat.jpeg', '/content/drive/MyDrive/Images/car1.jpg', '/content/drive/MyDrive/Images/flower1.jpg', '/content/drive/MyDrive/Images/bird1.jpg']\n",
        "images = [Image.open(path) for path in image_paths]\n",
        "image_tensors = torch.stack([transform(image) for image in images]).to('cuda')  # Move the input tensor to GPU if available\n",
        "\n",
        "# Run forward pass with FP32 model and monitor memory usage\n",
        "print(\"Memory usage before FP32 forward pass:\", get_gpu_memory_usage(), \"MiB\")\n",
        "with torch.no_grad():\n",
        "    logits_fp32 = model_fp32.encode_image(image_tensors)\n",
        "print(\"Memory usage after FP32 forward pass:\", get_gpu_memory_usage(), \"MiB\")\n",
        "\n",
        "# Run forward pass with FP16 model and monitor memory usage\n",
        "print(\"Memory usage before FP16 forward pass:\", get_gpu_memory_usage(), \"MiB\")\n",
        "with torch.no_grad():\n",
        "    logits_fp16 = model_fp16.encode_image(image_tensors.half())\n",
        "print(\"Memory usage after FP16 forward pass:\", get_gpu_memory_usage(), \"MiB\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
